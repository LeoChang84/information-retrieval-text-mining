{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "file_path = 'https://ceiba.ntu.edu.tw/course/35d27d/content/28.txt'\n",
    "directory = glob.glob('*.txt')\n",
    "if len(directory) == 0:\n",
    "    os.system('wget' + ' ' + file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenize: ['and', 'yugoslav', 'authorities', 'are', 'planning', 'the', 'arrest', 'of', 'eleven', 'coal', 'miners', 'and', 'two', 'opposition', 'politicians', 'on', 'suspicion', 'of', 'sabotage', 'that', 'in', 'connection', 'with', 'strike', 'action', 'against', 'president', 'slobodan', 'milosevic', 'you', 'are', 'listening', 'to', 'bbc', 'news', 'for', 'the', 'world']\n"
     ]
    }
   ],
   "source": [
    "# Tokenization and Lowercasing everything\n",
    "from nltk import word_tokenize\n",
    "\n",
    "tokens = []\n",
    "\n",
    "with open('28.txt', 'r') as f_in:\n",
    "    for line in f_in:\n",
    "        line = line.strip()\n",
    "        tokens.extend(word_tokenize(line))\n",
    "        tokens=[token.lower() for token in tokens if token.isalpha()]\n",
    "print('Tokenize: {}'.format(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Porter's result: ['and', 'yugoslav', 'author', 'are', 'plan', 'the', 'arrest', 'of', 'eleven', 'coal', 'miner', 'and', 'two', 'opposit', 'politician', 'on', 'suspicion', 'of', 'sabotag', 'that', 'in', 'connect', 'with', 'strike', 'action', 'against', 'presid', 'slobodan', 'milosev', 'you', 'are', 'listen', 'to', 'bbc', 'news', 'for', 'the', 'world']\n"
     ]
    }
   ],
   "source": [
    "# Stemming using Porterâ€™s algorithm.\n",
    "from nltk.stem.porter import *\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "singles = [stemmer.stem(t) for t in tokens]\n",
    "print('Porter\\'s result: {}'.format(singles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['yugoslav', 'author', 'plan', 'arrest', 'eleven', 'coal', 'miner', 'two', 'opposit', 'politician', 'suspicion', 'sabotag', 'connect', 'strike', 'action', 'presid', 'slobodan', 'milosev', 'listen', 'bbc', 'news', 'world']\n",
      "['with', 'you', 'to', 'in', 'against', 'that', 'and', 'for', 'are', 'the', 'of', 'on']\n"
     ]
    }
   ],
   "source": [
    "# Stopword removal.\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stops = set(stopwords.words('english'))\n",
    "results = ([s for s in singles if s not in stops])\n",
    "print(results)\n",
    "print(list(set(singles) - set(results)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Save the result as a txt file.\n",
    "\n",
    "directory = glob.glob('result.txt')\n",
    "\n",
    "if len(directory) == 0:\n",
    "    with open('result.txt', 'w+') as f_out:\n",
    "        f_out.write('\\n'.join([str(r) for r in results]))\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
