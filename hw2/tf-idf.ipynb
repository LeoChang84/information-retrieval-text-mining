{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "filename = 'IRTM.zip'\n",
    "if not os.path.isfile(filename):\n",
    "    os.system('wget https://ceiba.ntu.edu.tw/course/1d2744/hw/IRTM.zip')\n",
    "os.system('7z x -PIRTM2019 ' + filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(filename):\n",
    "    from nltk import word_tokenize\n",
    "    from nltk.stem.porter import PorterStemmer\n",
    "    from nltk.corpus import stopwords\n",
    "    \n",
    "    tokens = []\n",
    "    with open(filename, 'r') as f_in:\n",
    "        for line in f_in:\n",
    "            line = line.strip()\n",
    "            tokens.extend(word_tokenize(line))\n",
    "            tokens = [token.lower() for token in tokens if token.isalpha()]\n",
    "#     print('Tokenize: {}'.format(tokens))\n",
    "    stemmer = PorterStemmer()\n",
    "    singles = [stemmer.stem(t) for t in tokens]\n",
    "#     print('Porter\\'s result: {}'.format(singles))\n",
    "    stops = set(stopwords.words('english'))\n",
    "    results = ([s for s in singles if s not in stops])\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocabulary_dict(count_list):\n",
    "    import pickle\n",
    "    if not os.path.isfile('dictionary.txt'):\n",
    "        df = {}\n",
    "        for count in count_list:\n",
    "            _list = list()\n",
    "            for word in count:\n",
    "                if word not in df and word not in _list:\n",
    "                    df[word] = 1\n",
    "                    _list.append(word)\n",
    "                elif word in df and word not in _list:\n",
    "                    df[word] += 1\n",
    "                    _list.append(word)\n",
    "        with open('dictionary.txt', 'w') as f_out:\n",
    "            for i, key in enumerate(sorted(df.keys()), start=1):\n",
    "                f_out.write(str(i) + ' ' + key + ' ' + str(df[key]) + '\\n')\n",
    "        with open('dictionary.pkl', 'wb') as pkl_out:\n",
    "            pickle.dump(df, pkl_out)\n",
    "    with open('dictionary.pkl', 'rb') as pkl_in:\n",
    "        df = pickle.load(pkl_in)\n",
    "        return df\n",
    "    \n",
    "def tf(word, count):\n",
    "    return count[word] / sum(count.values())\n",
    "\n",
    "def n_containing(word, count_list):\n",
    "    return get_vocabulary_dict(count_list)[word]\n",
    "\n",
    "def idf(word, count_list):\n",
    "    return math.log(len(count_list) / n_containing(word, count_list))\n",
    "\n",
    "def tfidf(word, count, count_list):\n",
    "    return tf(word, count) * idf(word, count_list)\n",
    "\n",
    "def cos_sim(docx, docy):\n",
    "    from scipy import spatial\n",
    "    import numpy as np\n",
    "    key_union = list(set(docx.keys()) | set(docy.keys()))\n",
    "    result = 1 - spatial.distance.cosine([docx[k] if k in docx else 0 for k in key_union], [docy[k] if k in docy else 0 for k in key_union])\n",
    "    print(result)\n",
    "\n",
    "def get_int(filename):\n",
    "    start, end = '/', '.'\n",
    "    return int(filename[filename.find(start)+1:filename.find(end)])\n",
    "\n",
    "def save_vector_file(filename, sorted_document):\n",
    "    if not os.path.isfile('vector_files/' + filename + 'txt'):\n",
    "        with open('vector_files/' + filename + '.txt', 'w') as f:\n",
    "            f.write(str(len(sorted_document)) + '\\n')\n",
    "            f.write('t_index tf-idf' + '\\n')\n",
    "            for t_index, tf_idf in sorted_document:\n",
    "                f.write(str(t_index) + ' ' + str(tf_idf) + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import math\n",
    "from collections import Counter\n",
    "\n",
    "directory = glob.glob('IRTM/*.txt')\n",
    "countList = []\n",
    "for d in sorted(directory, key=get_int):\n",
    "    tokens = preprocess(d)\n",
    "    count = Counter(tokens)\n",
    "    countList.append(count)\n",
    "tf_documents = []\n",
    "sorted_documents = []\n",
    "for i, count in enumerate(countList):\n",
    "    word2id = {key: index for index, key in enumerate(sorted(get_vocabulary_dict(countList).keys()), start=1)}\n",
    "    scores = {word2id[word]: tfidf(word, count, countList) for word in count}\n",
    "    sorted_words = sorted(scores.items(), key=lambda x: x[0])\n",
    "    save_vector_file(str(i+1), sorted_words)\n",
    "    tf_documents.append(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.17760931780045264\n"
     ]
    }
   ],
   "source": [
    "cos_sim(tf_documents[0], tf_documents[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
