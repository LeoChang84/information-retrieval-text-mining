{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_testing_set(training_set):\n",
    "    from glob import glob\n",
    "    \n",
    "    directory = glob('IRTM/*')\n",
    "    for l in training_set:\n",
    "        directory.remove(l)\n",
    "    return directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stop_word_list():\n",
    "    from nltk.corpus import stopwords\n",
    "    return set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(filename):\n",
    "    from nltk import word_tokenize\n",
    "    from nltk.stem.porter import PorterStemmer\n",
    "    from nltk.corpus import stopwords\n",
    "    \n",
    "    tokens = []\n",
    "    with open(filename, 'r') as f_in:\n",
    "        for line in f_in:\n",
    "            line = line.strip()\n",
    "            tokens.extend(word_tokenize(line))\n",
    "            tokens = [token.lower() for token in tokens if token.isalpha()]\n",
    "#     print('Tokenize: {}'.format(tokens))\n",
    "    stemmer = PorterStemmer()\n",
    "    singles = [stemmer.stem(t) for t in tokens]\n",
    "#     print('Porter\\'s result: {}'.format(singles))\n",
    "    stops = set(stopwords.words('english'))\n",
    "    results = ([s for s in singles if s not in stops])\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocabulary_dict(count_list):\n",
    "    import pickle\n",
    "    import os\n",
    "    if not os.path.isfile('dictionary.txt'):\n",
    "        df = {}\n",
    "        for count in count_list:\n",
    "            _list = list()\n",
    "            for word in count:\n",
    "                if word not in df and word not in _list:\n",
    "                    df[word] = 1\n",
    "                    _list.append(word)\n",
    "                elif word in df and word not in _list:\n",
    "                    df[word] += 1\n",
    "                    _list.append(word)\n",
    "        with open('dictionary.txt', 'w') as f_out:\n",
    "            for i, key in enumerate(sorted(df.keys()), start=1):\n",
    "                f_out.write(str(i) + ' ' + key + ' ' + str(df[key]) + '\\n')\n",
    "        with open('dictionary.pkl', 'wb') as pkl_out:\n",
    "            pickle.dump(df, pkl_out)\n",
    "    with open('dictionary.pkl', 'rb') as pkl_in:\n",
    "        df = pickle.load(pkl_in)\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_extraction(df, classified_num, word2id):\n",
    "    import math\n",
    "    feature_dict, feature_voc = {}, {}\n",
    "    for key in word2id:\n",
    "        sigma_k = len(df[df['wordIdx'] == word2id[key]])\n",
    "        T_P = sigma_k / 195\n",
    "        N_P = 1 - T_P\n",
    "        score = 0\n",
    "        for i in range(1, classified_num+1):\n",
    "            k = (len(df[(df['classIdx'] == str(i)) & (df['wordIdx'] == word2id[key])]))\n",
    "            E_1 = T_P * (15/195)\n",
    "            E_0 = N_P * (15/195)\n",
    "            if E_1 == 0 or E_0 == 0: continue\n",
    "            score += ((k-E_1)**(2))/E_1 + ((15-k-E_0)**(2))/E_0\n",
    "#             print(k, E_1, 15-k, E_0)\n",
    "        if word2id[key] not in feature_dict:\n",
    "            feature_dict[word2id[key]] = score\n",
    "        else:\n",
    "            if feature_dict[word2id[key]] < score: feature_dict[word2id[key]] = score\n",
    "#         print(key ,score)\n",
    "    #     break\n",
    "    feature_voc = [k for k, v in sorted(feature_dict.items(), key=lambda item: item[1], reverse=True)]\n",
    "    return feature_voc[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MNB(df, feature_500_voc):\n",
    "    #Using dictionaries for greater speed\n",
    "    df_dict = df.to_dict()\n",
    "    new_dict = {}\n",
    "    prediction = []\n",
    "    #new_dict = {docIdx : {wordIdx: count},....}\n",
    "    for idx in range(len(df_dict['docIdx'])):\n",
    "        docIdx = df_dict['docIdx'][idx]\n",
    "        wordIdx = df_dict['wordIdx'][idx]\n",
    "        count = df_dict['count'][idx]\n",
    "        try: \n",
    "            new_dict[docIdx][wordIdx] = count \n",
    "        except:\n",
    "            new_dict[df_dict['docIdx'][idx]] = {}\n",
    "            new_dict[docIdx][wordIdx] = count\n",
    "    #Calculating the scores for each doc\n",
    "    for docIdx in new_dict:\n",
    "        score_dict = {}\n",
    "        #Creating a probability row for each class\n",
    "        for classIdx in range(1,classified_num+1):\n",
    "            classIdx = str(classIdx)\n",
    "            score_dict[classIdx] = 1\n",
    "            #For each word:\n",
    "            for wordIdx in new_dict[docIdx]:\n",
    "                if wordIdx in feature_500_voc:\n",
    "                    try:\n",
    "                        probability=Pr_dict[wordIdx][classIdx]         \n",
    "                        score_dict[classIdx]+=np.log(probability)\n",
    "                    except:\n",
    "                        #Missing V will have log(1+0)*log(a/16689)=0 \n",
    "                        score_dict[classIdx] += 0                        \n",
    "        #Get class with max probabilty for the given docIdx \n",
    "        max_score = max(score_dict, key=score_dict.get)\n",
    "        prediction.append(max_score)\n",
    "        \n",
    "    return prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "countList, df_arrays = [], []\n",
    "total, classified_num = 0, 0\n",
    "training_set = []\n",
    "\n",
    "#Training label\n",
    "with open('training.txt') as train_label:\n",
    "    for line in train_label:\n",
    "        line = line.strip().split(' ')\n",
    "        tokens = []\n",
    "        for i in line[1:]:\n",
    "            training_set.append('IRTM/{}.txt'.format(i))\n",
    "            tokens += preprocess('IRTM/{}.txt'.format(i))\n",
    "            count = Counter(tokens)\n",
    "            countList.append(count)\n",
    "    testing_set = generate_testing_set(training_set)\n",
    "    word2id = {key: index for index, key in enumerate(sorted(get_vocabulary_dict(countList).keys()), start=1)}\n",
    "    train_label.seek(0, 0)\n",
    "    for line in train_label:\n",
    "        line = line.strip().split(' ')\n",
    "        classified_num += 1\n",
    "        for i in line[1:]:\n",
    "            tokens = []\n",
    "            tokens = preprocess('IRTM/{}.txt'.format(i))\n",
    "            count = Counter(tokens)\n",
    "            for key in count:\n",
    "                df_array = []\n",
    "                df_array.append(int(i))\n",
    "                df_array.append(word2id[key])\n",
    "                df_array.append(count[key])\n",
    "                df_array.append(line[0])\n",
    "                df_arrays.append(df_array)\n",
    "    df = pd.DataFrame(df_arrays, columns=['docIdx', 'wordIdx', 'count', 'classIdx'])\n",
    "\n",
    "feature_500_voc = feature_extraction(df, classified_num, word2id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Alpha value for smoothing\n",
    "a = 1\n",
    "V = len(feature_500_voc)\n",
    "#Calculate probability of each word based on class\n",
    "pb_ij = df.groupby(['classIdx','wordIdx'])\n",
    "pb_j = df.groupby(['classIdx'])\n",
    "Pr =  (pb_ij['count'].sum() + a) / (pb_j['count'].sum() + V)\n",
    "#Unstack series\n",
    "Pr = Pr.unstack()\n",
    "\n",
    "#Replace NaN or columns with 0 as word count with a/(count+|V|+1)\n",
    "for c in range(1,classified_num+1):\n",
    "    Pr.loc[str(c),:] = Pr.loc[str(c),:].fillna(a/(pb_j['count'].sum()[str(c)] + V))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_array = []\n",
    "for line in testing_set:\n",
    "    df_arrays = []\n",
    "    tokens = []\n",
    "    tokens = preprocess(line)\n",
    "    count = Counter(tokens)\n",
    "    for key in count:\n",
    "        if key in word2id:\n",
    "            df_array = []\n",
    "            df_array.append(int(i))\n",
    "            df_array.append(word2id[key])\n",
    "            df_array.append(count[key])\n",
    "            df_array.append(line[0])\n",
    "            df_arrays.append(df_array)\n",
    "    \n",
    "    df = pd.DataFrame(df_arrays, columns=['docIdx', 'wordIdx', 'count', 'classIdx'])\n",
    "    predict = MNB(df, feature_500_voc)\n",
    "    test_array.append([line.split('/')[1].split('.')[0], predict[0]])\n",
    "# print(test_array)\n",
    "df = pd.DataFrame(sorted(test_array), columns=['Id','Value'])\n",
    "# print(sorted(test_array))\n",
    "df.to_csv('HW3.csv', index =False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
